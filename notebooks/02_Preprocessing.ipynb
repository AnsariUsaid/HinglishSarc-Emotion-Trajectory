{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HinglishSarc - Data Preprocessing\n",
        "\n",
        "**Week 1, Day 3-4: Data Preprocessing & Train/Val/Test Split**\n",
        "\n",
        "This notebook:\n",
        "1. Preprocesses Hinglish text (normalization, cleaning)\n",
        "2. Creates train/val/test splits (70/15/15)\n",
        "3. Saves preprocessed datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('../scripts')\n",
        "from preprocess_utils import HinglishPreprocessor, split_into_sentences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print('✓ Imports successful!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sarcasm dataset\n",
        "sarc_df = pd.read_csv('../data/raw/sarcasm_hinghlish_dataset.csv')\n",
        "print(f'Sarcasm dataset loaded: {sarc_df.shape}')\n",
        "print(f'Columns: {sarc_df.columns.tolist()}')\n",
        "sarc_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = HinglishPreprocessor(\n",
        "    lowercase=True,\n",
        "    remove_urls=True,\n",
        "    remove_mentions=True,\n",
        "    remove_hashtags=False,  # Keep hashtags as they may indicate sarcasm\n",
        "    normalize_whitespace=True,\n",
        "    preserve_emojis=True,  # Keep emojis as they carry emotional information\n",
        "    remove_punctuation=False  # Keep punctuation for sentence splitting\n",
        ")\n",
        "\n",
        "print('✓ Preprocessor initialized')\n",
        "print('Configuration:')\n",
        "print(f'  - Lowercase: {preprocessor.lowercase}')\n",
        "print(f'  - Remove URLs: {preprocessor.remove_urls}')\n",
        "print(f'  - Remove mentions: {preprocessor.remove_mentions}')\n",
        "print(f'  - Preserve emojis: {preprocessor.preserve_emojis}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Preprocessing on Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on sample texts\n",
        "print('Testing preprocessing on sample texts:\\n')\n",
        "for idx in [0, 1, 5, 10]:\n",
        "    original = sarc_df.iloc[idx]['text']\n",
        "    cleaned = preprocessor.preprocess(original)\n",
        "    sentences = split_into_sentences(cleaned)\n",
        "    \n",
        "    print(f'Sample {idx}:')\n",
        "    print(f'  Original:  {original}')\n",
        "    print(f'  Cleaned:   {cleaned}')\n",
        "    print(f'  Sentences: {len(sentences)} - {sentences}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocess All Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess all texts\n",
        "print('Preprocessing all texts...')\n",
        "sarc_df['text_cleaned'] = sarc_df['text'].apply(preprocessor.preprocess)\n",
        "\n",
        "# Calculate sentence counts for trajectory modeling\n",
        "sarc_df['sentence_count'] = sarc_df['text_cleaned'].apply(\n",
        "    lambda x: len(split_into_sentences(x))\n",
        ")\n",
        "\n",
        "# Calculate cleaned text length\n",
        "sarc_df['cleaned_length'] = sarc_df['text_cleaned'].str.len()\n",
        "sarc_df['cleaned_word_count'] = sarc_df['text_cleaned'].str.split().str.len()\n",
        "\n",
        "print(f'✓ Preprocessing complete!')\n",
        "print(f'\\nCleaned dataset shape: {sarc_df.shape}')\n",
        "print(f'Average sentences per text: {sarc_df[\"sentence_count\"].mean():.2f}')\n",
        "sarc_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyze Preprocessing Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare original vs cleaned\n",
        "print('=== PREPROCESSING IMPACT ===')\n",
        "print(f'Average length reduction: {(1 - sarc_df[\"cleaned_length\"].mean() / sarc_df[\"text\"].str.len().mean()) * 100:.2f}%')\n",
        "print(f'\\nSentence count distribution:')\n",
        "print(sarc_df['sentence_count'].value_counts().sort_index())\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Sentence count distribution\n",
        "sarc_df['sentence_count'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='#3498db')\n",
        "ax[0].set_title('Sentence Count Distribution (for Trajectory Modeling)')\n",
        "ax[0].set_xlabel('Number of Sentences')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_xlim(-0.5, 10.5)\n",
        "\n",
        "# Length comparison\n",
        "ax[1].scatter(sarc_df['text'].str.len(), sarc_df['cleaned_length'], alpha=0.3, color='#e74c3c')\n",
        "ax[1].plot([0, 500], [0, 500], 'k--', alpha=0.5, label='No change line')\n",
        "ax[1].set_title('Text Length: Original vs Cleaned')\n",
        "ax[1].set_xlabel('Original Length')\n",
        "ax[1].set_ylabel('Cleaned Length')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlim(0, 500)\n",
        "ax[1].set_ylim(0, 500)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/figures/preprocessing_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Train/Val/Test Splits\n",
        "\n",
        "Split: 70% train, 15% validation, 15% test\n",
        "\n",
        "Stratified by sarcasm label to maintain class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First split: 70% train, 30% temp (val + test)\n",
        "train_df, temp_df = train_test_split(\n",
        "    sarc_df,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=sarc_df['label']\n",
        ")\n",
        "\n",
        "# Second split: 50% val, 50% test from temp (15% each of total)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    stratify=temp_df['label']\n",
        ")\n",
        "\n",
        "print('=== DATASET SPLITS ===')\n",
        "print(f'Train: {len(train_df)} samples ({len(train_df)/len(sarc_df)*100:.1f}%)')\n",
        "print(f'Val:   {len(val_df)} samples ({len(val_df)/len(sarc_df)*100:.1f}%)')\n",
        "print(f'Test:  {len(test_df)} samples ({len(test_df)/len(sarc_df)*100:.1f}%)')\n",
        "print(f'Total: {len(train_df) + len(val_df) + len(test_df)} samples')\n",
        "\n",
        "# Check stratification\n",
        "print('\\n=== LABEL DISTRIBUTION ===')\n",
        "print(f'Train sarcasm ratio: {train_df[\"label\"].mean():.2%}')\n",
        "print(f'Val sarcasm ratio:   {val_df[\"label\"].mean():.2%}')\n",
        "print(f'Test sarcasm ratio:  {test_df[\"label\"].mean():.2%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize split distributions\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "splits = [('Train', train_df), ('Val', val_df), ('Test', test_df)]\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "\n",
        "for idx, (name, df) in enumerate(splits):\n",
        "    counts = df['label'].value_counts().sort_index()\n",
        "    ax[idx].bar(['Non-Sarcastic', 'Sarcastic'], counts.values, color=colors)\n",
        "    ax[idx].set_title(f'{name} Set (n={len(df)})')\n",
        "    ax[idx].set_ylabel('Count')\n",
        "    ax[idx].set_ylim(0, max(counts.values) * 1.1)\n",
        "    \n",
        "    # Add percentage labels\n",
        "    for i, v in enumerate(counts.values):\n",
        "        ax[idx].text(i, v + 50, f'{v/len(df)*100:.1f}%', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/figures/train_val_test_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Preprocessed Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select relevant columns\n",
        "columns_to_save = ['text', 'text_cleaned', 'label', 'sentence_count', \n",
        "                   'cleaned_length', 'cleaned_word_count']\n",
        "\n",
        "# Save to CSV\n",
        "train_df[columns_to_save].to_csv('../data/processed/train.csv', index=False)\n",
        "val_df[columns_to_save].to_csv('../data/processed/val.csv', index=False)\n",
        "test_df[columns_to_save].to_csv('../data/processed/test.csv', index=False)\n",
        "\n",
        "print('✓ Datasets saved to data/processed/')\n",
        "print('  - train.csv')\n",
        "print('  - val.csv')\n",
        "print('  - test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Preprocessing Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = {\n",
        "    'Split': ['Train', 'Val', 'Test', 'Total'],\n",
        "    'Samples': [len(train_df), len(val_df), len(test_df), len(sarc_df)],\n",
        "    'Sarcastic': [\n",
        "        train_df['label'].sum(),\n",
        "        val_df['label'].sum(),\n",
        "        test_df['label'].sum(),\n",
        "        sarc_df['label'].sum()\n",
        "    ],\n",
        "    'Non-Sarcastic': [\n",
        "        (train_df['label']==0).sum(),\n",
        "        (val_df['label']==0).sum(),\n",
        "        (test_df['label']==0).sum(),\n",
        "        (sarc_df['label']==0).sum()\n",
        "    ],\n",
        "    'Avg Sentences': [\n",
        "        train_df['sentence_count'].mean(),\n",
        "        val_df['sentence_count'].mean(),\n",
        "        test_df['sentence_count'].mean(),\n",
        "        sarc_df['sentence_count'].mean()\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary)\n",
        "print('\\n=== PREPROCESSING SUMMARY ===')\n",
        "print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Day 3-4 Checkpoint\n",
        "\n",
        "**Completed:**\n",
        "- ✓ Text preprocessing pipeline implemented\n",
        "- ✓ Hinglish normalization (URLs, mentions, whitespace)\n",
        "- ✓ Sentence splitting for trajectory modeling\n",
        "- ✓ Train/val/test splits (70/15/15) with stratification\n",
        "- ✓ Preprocessed datasets saved\n",
        "\n",
        "**Key Statistics:**\n",
        "- Train: 6,715 samples (70%)\n",
        "- Val: 1,439 samples (15%)\n",
        "- Test: 1,439 samples (15%)\n",
        "- Stratified sarcasm distribution maintained\n",
        "- Average ~2-3 sentences per text for trajectory modeling\n",
        "\n",
        "**Next Steps:** Day 5-7 - Baseline Models"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}